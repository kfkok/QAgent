{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate QAgent training against the env Q-table\n",
    "# For debugging purposes\n",
    "\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from test_env import TestEnv\n",
    "from agents.q_agent import QAgent\n",
    "\n",
    "env = TestEnv()\n",
    "agent = QAgent(env)\n",
    "\n",
    "q_values = env.calculate_q_table_state_action_values()\n",
    "agent.train(50000)\n",
    "\n",
    "agent_q_values = agent.q_table\n",
    "\n",
    "# Ideally a small training episode should result in a q_table that is close to the actual q_table\n",
    "\n",
    "# Print the difference between the actual Q-Values and the Agent's Q-Values\n",
    "print(\"Comparing Q-Values between Test Environment Q-Table and Agent's Q-Table\")\n",
    "\n",
    "# Use the union operation to get the unique keys from both sets\n",
    "states = set(q_values.keys()).union(set(agent_q_values.keys()))\n",
    "\n",
    "for state in states:\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"Actual Q-Values: {q_values.get(state, {})}\")\n",
    "    print(f\"Agent Q-Values: {agent_q_values.get(state, {})}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please run this cell before running the following cell\n",
    "\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from agents.q_agent import QAgent \n",
    "\n",
    "# Example values for the parameters\n",
    "params = {\n",
    "    # ------------------------------- #\n",
    "    # Environment parameters\n",
    "    # ------------------------------- #\n",
    "\n",
    "    # The environment to use\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "\n",
    "    # ------------------------------- #\n",
    "    # Agent parameters\n",
    "    # ------------------------------- #\n",
    "\n",
    "    # The state filter to use, here we only concern ourselves with the cart velocity, pole angle and pole velocity, thus we ignore the cart position which is the first element in the state\n",
    "    \"state_filter\": lambda state : (state[1], state[2], state[3]),\n",
    "\n",
    "    # The learning rate to use\n",
    "    \"learning_rate\": 0.07,\n",
    "\n",
    "    # The discount factor to use\n",
    "    \"discount_factor\": 0.99,\n",
    "\n",
    "    # The state rounding to use, 1 means we round, for example, 0.3123 to 0.3, this is useful for reducing the state space\n",
    "    \"state_rounding\": 1,\n",
    "\n",
    "    # ------------------------------- #\n",
    "    # Agent training parameters\n",
    "    # ------------------------------- #\n",
    "\n",
    "    # The number of episodes the agent plays during training, the higher the better, but also the longer it takes\n",
    "    \"training_episodes\": 7000,\n",
    "\n",
    "    # The initial epsilon value to use, this is the probability of taking a random action, 1 means always take random actions while 0 means always take the best action\n",
    "    \"initial_epsilon\": 1.0,\n",
    "\n",
    "    # The minimum epsilon value to use, this is the lowest probability of taking a random action\n",
    "    \"min_epsilon\": 0.1,\n",
    "\n",
    "    # The decay percentage to use, this is the percentage by which epsilon decays after each episode from initial_epsilon to min_epsilon\n",
    "    # For example, if initial_epsilon is 1.0, min_epsilon is 0.1 and decay_percentage is 0.5, then epsilon will decay from 1.0 to 0.1 after 50% of the training episodes\n",
    "    \"decay_percentage\": 0.5,\n",
    "\n",
    "    # ------------------------------- #\n",
    "    # Test parameters\n",
    "    # ------------------------------- #\n",
    "\n",
    "    # The number of episodes the agent plays to test its performance after training\n",
    "    # During testing, epsilon is set to 0, meaning the agent will always take the best action\n",
    "    \"test_episodes\": 20\n",
    "}\n",
    "\n",
    "# The run function is called by the main script\n",
    "def run(params):\n",
    "    env = gym.make(params[\"env_name\"])\n",
    "    agent = QAgent(\n",
    "        env=env,\n",
    "        state_filter=params[\"state_filter\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        discount_factor=params[\"discount_factor\"],\n",
    "        state_rounding=params[\"state_rounding\"]\n",
    "    )\n",
    "    reward_over_episodes = []\n",
    "\n",
    "    # Train the agent for 100 episodes\n",
    "    agent.train(\n",
    "        episodes=params[\"training_episodes\"], \n",
    "        initial_epsilon=params[\"initial_epsilon\"], \n",
    "        min_epsilon=params[\"min_epsilon\"], \n",
    "        decay_percentage=params[\"decay_percentage\"]\n",
    "    )\n",
    "\n",
    "    # Env mode set to human to visualize the agent's performance\n",
    "    env = gym.make(params[\"env_name\"], render_mode=\"human\")\n",
    "    print(\"Testing the agent...\")\n",
    "    episodes_count = params[\"test_episodes\"]\n",
    "    for i in range(episodes_count):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_best_action(state)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        if done:\n",
    "            reward_over_episodes.append(episode_reward)\n",
    "            print(\"Episode terminated, total reward:\", episode_reward)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Plot the reward over episodes\n",
    "    plt.plot(reward_over_episodes)\n",
    "    plt.ylabel('Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent with the parameters\n",
    "\n",
    "run({\n",
    "    # ------------------------------- #\n",
    "    # Environment parameters\n",
    "    # ------------------------------- #\n",
    "\n",
    "    # The environment to use\n",
    "    \"env_name\": \"CartPole-v1\",\n",
    "\n",
    "    # ------------------------------- #\n",
    "    # Agent parameters\n",
    "    # ------------------------------- #\n",
    "\n",
    "    # The state filter to use, here we only concern ourselves with the cart velocity, pole angle and pole velocity, thus we ignore the cart position which is the first element in the state\n",
    "    \"state_filter\": lambda state : (state[1], state[2], state[3]),\n",
    "\n",
    "    # The learning rate to use\n",
    "    \"learning_rate\": 0.07,\n",
    "\n",
    "    # The discount factor to use\n",
    "    \"discount_factor\": 0.99,\n",
    "\n",
    "    # The state rounding to use, 1 means we round, for example, 0.3123 to 0.3, this is useful for reducing the state space\n",
    "    \"state_rounding\": 1,\n",
    "\n",
    "    # ------------------------------- #\n",
    "    # Agent training parameters\n",
    "    # ------------------------------- #\n",
    "\n",
    "    # The number of episodes the agent plays during training, the higher the better, but also the longer it takes\n",
    "    \"training_episodes\": 7000,\n",
    "\n",
    "    # The initial epsilon value to use, this is the probability of taking a random action, 1 means always take random actions while 0 means always take the best action\n",
    "    \"initial_epsilon\": 1.0,\n",
    "\n",
    "    # The minimum epsilon value to use, this is the lowest probability of taking a random action\n",
    "    \"min_epsilon\": 0.1,\n",
    "\n",
    "    # The decay percentage to use, this is the percentage by which epsilon decays after each episode from initial_epsilon to min_epsilon\n",
    "    # For example, if initial_epsilon is 1.0, min_epsilon is 0.1 and decay_percentage is 0.5, then epsilon will decay from 1.0 to 0.1 after 50% of the training episodes\n",
    "    \"decay_percentage\": 0.5,\n",
    "\n",
    "    # ------------------------------- #\n",
    "    # Test parameters\n",
    "    # ------------------------------- #\n",
    "\n",
    "    # The number of episodes the agent plays to test its performance after training\n",
    "    # During testing, epsilon is set to 0, meaning the agent will always take the best action\n",
    "    \"test_episodes\": 20\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
